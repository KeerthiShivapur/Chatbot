LangChain Vector Store & Retriever Code â€” Complete Function & Keyword Notes
1ï¸âƒ£ from langchain.document_loaders import PyPDFLoader

Loads PDF documents and splits them into readable text chunks.

Converts each page into a LangChain Document object (with metadata).

ğŸ—£ Used to extract textual data from PDFs for embedding and retrieval.

2ï¸âƒ£ from langchain.text_splitter import RecursiveCharacterTextSplitter

Splits long documents into smaller overlapping text chunks (tokens).

Parameters:

chunk_size â†’ max characters per chunk.

chunk_overlap â†’ overlap between consecutive chunks to preserve context.

ğŸ§  Chunking prevents token overflow and maintains semantic continuity.

3ï¸âƒ£ from langchain.embeddings import HuggingFaceEmbeddings

Creates numerical vector embeddings using HuggingFace transformer models.

Converts text into high-dimensional vectors that capture semantic meaning.

ğŸ—£ Essential for building a searchable vector database.

4ï¸âƒ£ from langchain.vectorstores import FAISS

Imports the FAISS (Facebook AI Similarity Search) integration.

FAISS stores all embeddings in a vector index and allows fast similarity search.

ğŸ§  Core engine for retrieval-based Q&A.

5ï¸âƒ£ from langchain.vectorstores import Chroma

Optional vector store alternative to FAISS.

Supports persistent storage and filtering capabilities.

ğŸ—£ Used when you need to save the index permanently (e.g., on disk).

6ï¸âƒ£ from langchain.llms import OpenAI / from langchain_groq import ChatGroq

Loads the LLM model used to generate final answers.

Either OpenAI or Groq can be used as the language reasoning component.

ğŸ§  Embeddings â†’ retriever â†’ model â†’ answer.

7ï¸âƒ£ docs = loader.load()

Loads all pages from the given PDF(s) and returns a list of Document objects.

Each document contains page_content and metadata.

ğŸ—£ Transforms unstructured files into structured LangChain docs.

8ï¸âƒ£ text_splitter.split_documents(docs)

Applies chunking to break large docs into smaller, overlapping segments.

These chunks become the units for embedding and retrieval.

ğŸ§  Improves retrieval accuracy by keeping context size manageable.

9ï¸âƒ£ embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

Loads a specific sentence embedding model from HuggingFace.

Creates a reusable embedding generator.

ğŸ—£ Encodes each chunk into a 384-dimensional semantic vector.

ğŸ”Ÿ db = FAISS.from_documents(docs, embeddings)

Builds a FAISS index directly from documents and their embeddings.

Automatically computes and stores embeddings in vector form.

ğŸ§  Initializes your searchable knowledge base.

11ï¸âƒ£ db.save_local("faiss_index")

Saves the FAISS index to disk (for later reuse).

Creates metadata and binary index files.

ğŸ—£ Used for persistence and quick reloads.

12ï¸âƒ£ db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

Reloads the FAISS index from disk.

Reconnects it to the embedding model for new queries.

ğŸ§  Lets you reuse trained indices without recomputing embeddings.

13ï¸âƒ£ query = "What is the purpose of this research paper?"

Example user query string to retrieve relevant document chunks.

ğŸ—£ Used to test the retriever.

14ï¸âƒ£ docs = db.similarity_search(query, k=3)

Performs a vector similarity search.

Returns the top-3 chunks whose embeddings are most similar to the query vector.

ğŸ§  Key step of retrieval â€” finds the most semantically related context.

15ï¸âƒ£ from langchain.chains import RetrievalQA

A LangChain chain combining a retriever + LLM.

Automates the retrieval + question answering process.

ğŸ—£ Implements the RAG (Retrieval-Augmented Generation) pattern.

16ï¸âƒ£ qa = RetrievalQA.from_chain_type(llm=model, chain_type="stuff", retriever=db.as_retriever())

Builds a RetrievalQA pipeline:

llm â†’ model used for answer generation.

retriever â†’ database (FAISS/Chroma) that fetches context.

chain_type="stuff" â†’ means all retrieved chunks are concatenated (â€œstuffedâ€) into one context string.

ğŸ§  Simplifies RAG logic into one callable object.

17ï¸âƒ£ qa.run(query) / qa.invoke({"query": query})

Sends the query through the retriever â†’ LLM â†’ response flow.

Retrieves context, formats it, and generates the final answer.

ğŸ—£ Produces accurate, context-based responses.

18ï¸âƒ£ vectorstore.as_retriever(search_kwargs={"k": 5})

Converts a vectorstore (FAISS or Chroma) into a retriever with custom search parameters.

k defines how many top results to fetch for each query.

ğŸ§  Used when you need to fine-tune recall precision.

19ï¸âƒ£ persist_directory = "chroma_db"

Defines the folder where Chroma DB data will be stored.

Enables long-term persistence between runs.

ğŸ—£ Used with Chroma persistent databases.

20ï¸âƒ£ retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})

Configures the retriever to use similarity search and return top-4 similar chunks.

ğŸ§  Gives control over retrieval behavior.

21ï¸âƒ£ docs = retriever.get_relevant_documents(query)

Directly fetches relevant context documents without generating an answer.

Useful for debugging or previewing retrieved text.

ğŸ—£ Lets you inspect what content the model sees.

22ï¸âƒ£ prompt = PromptTemplate(template="Answer the following question using the context: {context}\nQuestion: {question}", input_variables=["context", "question"])

Defines a custom question-answering prompt.

{context} â†’ inserted retrieved document text.

{question} â†’ user query.

ğŸ§  Controls how context and question are formatted before sending to the model.

23ï¸âƒ£ chain = LLMChain(llm=model, prompt=prompt)

Creates a chain that uses the model and your custom prompt to generate the final answer.

ğŸ—£ Custom RAG pipeline for controlled responses.

24ï¸âƒ£ docs_content = " ".join([doc.page_content for doc in docs])

Joins retrieved chunks into a single text block for prompt input.

ğŸ§  Prepares the retrieved data for model input.

25ï¸âƒ£ chain.run(context=docs_content, question=query)

Executes the final chain â€” feeds retrieved context + query to LLM.

Returns the final generated answer.

ğŸ—£ Completes the RAG flow end-to-end.

ğŸ” Overall Flow Summary
PDF/Text Data â†’ Loader â†’ Text Splitter â†’ Embeddings â†’ Vector Store (FAISS/Chroma)
     â†“
User Query â†’ Retriever (similarity search)
     â†“
Top-K Chunks â†’ LLM (Groq/OpenAI)
     â†“
Prompt + Context â†’ Answer (context-aware response)

ğŸ’¬ Interview Summary (You Can Say This)

â€œIn my vector-based chatbot, I implemented Retrieval-Augmented Generation using LangChain.
I used PyPDFLoader to extract text from documents, RecursiveCharacterTextSplitter to chunk the text, and HuggingFaceEmbeddings to convert each chunk into semantic vectors.
These embeddings were stored in FAISS, which supports fast similarity searches.

For querying, I used RetrievalQA with a retriever built from FAISS.
When a user asks a question, the system performs similarity search, retrieves the most relevant chunks, and passes them to the LLM through a structured prompt.

This architecture enables context-aware question answering across documents â€” a classic RAG pipeline. Itâ€™s efficient, modular, and scalable for enterprise-grade search applications.â€