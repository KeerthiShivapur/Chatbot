LangChain Chatbot Code â€” Complete Function & Keyword Notes
1ï¸âƒ£ load_dotenv()

Loads environment variables (like API keys) from a .env file into Pythonâ€™s environment.

Keeps secrets secure â€” avoids hardcoding keys in code.

ğŸ—£ Interview Tip: â€œI used load_dotenv() for secure key management.â€

2ï¸âƒ£ os.getenv("GROQ_API_KEY")

Retrieves the value of the environment variable named GROQ_API_KEY.

If not found, it returns None.

ğŸ—£ Used to connect to Groq API securely.

3ï¸âƒ£ ChatGroq

LangChain wrapper for Groqâ€™s LLM models (like Gemma2-9b-It).

Handles chat-based interaction, request formatting, and API calls.

ğŸ—£ Used to initialize and communicate with Groq models.

4ï¸âƒ£ PromptTemplate

Allows dynamic prompt creation using placeholders (e.g., {chat_history}, {human_input}).

Helps structure the chatbotâ€™s role and response style.

ğŸ—£ Gives flexibility and cleaner prompt design.

5ï¸âƒ£ ConversationBufferMemory

Stores the conversation between user and chatbot in memory (RAM).

Preserves past messages to maintain continuity in multi-turn conversations.

ğŸ—£ Adds context retention to your chatbot.

6ï¸âƒ£ LLMChain

Connects the model, prompt, and memory into a single executable pipeline.

When you call .invoke(), it runs the prompt â†’ LLM â†’ memory update automatically.

ğŸ—£ Acts as the main logical chain for generating AI responses.

7ï¸âƒ£ AIMessage

Represents a message from the AI model in LangChainâ€™s message schema.

Used to simulate or store previous AI responses in the conversation list.

ğŸ—£ Useful when manually building chat context.

8ï¸âƒ£ HumanMessage

Represents a message from the human user in LangChain.

The content argument holds the text input (e.g., HumanMessage(content="Hi!")).

ğŸ—£ Used to represent each user query.

9ï¸âƒ£ model.invoke([...])

Sends a sequence of messages (HumanMessage, AIMessage) to the LLM for processing.

Returns a context-aware response considering previous messages.

ğŸ—£ Core function to get output from the model.

ğŸ”Ÿ ChatMessageHistory

Concrete implementation of BaseChatMessageHistory.

Stores chat messages in-memory (Python list).

Keeps full conversation logs for one session.

ğŸ—£ Used to maintain persistent conversation memory per user/session.

11ï¸âƒ£ BaseChatMessageHistory

Abstract base class defining how chat histories should behave.

Used as a type hint for get_session_history â€” ensures flexibility to use other backends (like Redis or DB).

ğŸ—£ Represents a general interface for chat history management.

12ï¸âƒ£ RunnableWithMessageHistory

Wraps a model or chain to automatically manage message history and sessions.

Uses a history retriever function (get_session_history) to load or create chat history for a specific session.

When .invoke() is called, it:

Fetches the correct sessionâ€™s history,

Appends new messages,

Sends full context to the model.

ğŸ—£ Adds automatic session-based memory management.

13ï¸âƒ£ store = {}

A Python dictionary that stores session IDs and their corresponding chat histories.

Key â†’ session_id

Value â†’ ChatMessageHistory() object

ğŸ—£ Acts as a local database for sessions.

14ï¸âƒ£ get_session_history(session_id: str)

Custom function that checks if a session ID exists in store.

If not, creates a new ChatMessageHistory instance.

Returns the history object for that session.

ğŸ—£ Handles per-session chat context retrieval.

15ï¸âƒ£ config = {"configurable": {"session_id": "chat1"}}

A dictionary used to pass runtime configuration to LangChain runnables.

session_id uniquely identifies a chat session.

Changing the ID starts a fresh memory.

ğŸ—£ Ensures each user/chat window is isolated.

16ï¸âƒ£ response = with_message_history.invoke([...], config=config)

Invokes the memory-wrapped model/chain with user input.

Automatically attaches and updates conversation history for that session.

ğŸ—£ Core step to generate replies with remembered context.

17ï¸âƒ£ response.content

Extracts the generated text response from the model output object.

LangChain responses include metadata, so .content gives only the main answer.

18ï¸âƒ£ ChatPromptTemplate

Advanced prompt class supporting multi-role messages (system, user, assistant).

Helps structure conversations for chat-based models.

ğŸ—£ Improves prompt readability and modularity.

19ï¸âƒ£ MessagesPlaceholder(variable_name="messages")

Placeholder in the prompt template that automatically injects chat history (messages).

Used for dynamic memory integration.

ğŸ—£ Links conversation memory into the prompt automatically.

20ï¸âƒ£ prompt = ChatPromptTemplate.from_messages([...])

Creates a prompt using multiple message roles (system, user, etc.).

Automatically structures how the prompt is sent to the model.

ğŸ—£ Defines system behavior + user message placeholders.

21ï¸âƒ£ chain = prompt | model

The | (pipe) operator connects components â€” forms a LangChain Runnable.

Input â†’ Prompt â†’ Model â†’ Output

ğŸ—£ Creates a clean and reusable LLM pipeline.

22ï¸âƒ£ input_messages_key="messages"

Tells RunnableWithMessageHistory where to insert messages in the input dictionary.

Links the memory key to the MessagesPlaceholder.

ğŸ—£ Ensures message history is correctly passed to the model.

23ï¸âƒ£ {language} in Prompt

A dynamic variable inside the prompt to switch response language.

Passed during .invoke() like:

response = chain.invoke({"messages": [...], "language": "kannada"})


Makes chatbot multilingual.

ğŸ—£ Adds flexibility for global users.

24ï¸âƒ£ chain.invoke({"messages":[HumanMessage(...)],"language":"kannada"})

Runs the entire prompt â†’ model pipeline with the specified messages and language variable.

Returns a response in the chosen language.

ğŸ—£ Demonstrates dynamic variable injection into prompt templates.

25ï¸âƒ£ verbose=True

Prints detailed logs showing the full prompt, chain flow, and outputs.

Useful for debugging prompt structure and model responses.

ğŸ—£ Used in development phase to trace flow.

26ï¸âƒ£ repsonse.content (typo)

Typo for response.content â€” extracts the text from the LLM response.

The variable should always be named correctly to avoid runtime errors.

ğŸ§© Overall Flow Summary
User Input â†’ HumanMessage â†’ (Session ID) â†’ get_session_history()
     â†“
Memory retrieved â†’ ChatPromptTemplate (with MessagesPlaceholder)
     â†“
Prompt formatted (with chat history + language)
     â†“
ChatGroq model invoked via RunnableWithMessageHistory
     â†“
Response generated + stored back in ChatMessageHistory

ğŸ’¬ Interview Summary

â€œI implemented a session-aware, multilingual chatbot using LangChain.
I used RunnableWithMessageHistory for automatic memory management, ChatMessageHistory for session-based storage, and ChatPromptTemplate with MessagesPlaceholder for dynamic prompt generation.
Each chat session is tracked using a unique session_id, and the chatbot can even respond in different languages using a {language} variable.
The system maintains context, remembers user details, and delivers consistent multi-turn dialogue â€” exactly like real-world AI assistants.â€

Would you like me to give you the next page as a